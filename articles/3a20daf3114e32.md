---
title: "python ã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã«ãŠã‘ã‚‹ tqdm-friendly ãª logging"
emoji: "ğŸ‘»"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Python"]
published: true
---

åå·¥å¤§ã‚¢ãƒ‰ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ 3 æ—¥ç›®ã§ã™ã€‚

# ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³

- ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã®é€²æ—çŠ¶æ³ã‚’ tqdm ã§ç®¡ç†ã—ã¦ã„ã¦ã€å­ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚‚ tqdm ã¨å¹²æ¸‰ã—ãªã„ log ã‚’è¡Œã„ãŸã„

# ã‚¢ã‚¤ãƒ‡ã‚¢

ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã§ã‚‚ `QueueHandler` ã§å…¨ãƒ­ã‚°ã‚’å…±æœ‰ `Queue` ã«é›†ç´„ã—ã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã® `QueueListener` + `TqdmLoggingHandler` ãŒ `tqdm.write` ã§ä¸€æ‹¬å‡ºåŠ›ã™ã‚‹ã“ã¨ã§ã€è¤‡æ•°ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ­ã‚°ãŒ tqdm ã®è¡¨ç¤ºã¨ç«¶åˆã›ãšã«å®‰å…¨ã«æµã‚Œã‚‹

# ãƒ­ã‚¸ãƒƒã‚¯

1. `TqdmLoggingHandler` ã§ `tqdm.write` ã‚’ä½¿ã£ã¦ãƒ­ã‚°ã‚’å‡ºã™ãƒãƒ³ãƒ‰ãƒ©ã‚’å®šç¾©ã—ã€ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’å£Šã•ãšã«ãƒ­ã‚°ã‚’åãå‡ºã›ã‚‹
2. `worker_init` ã‚’å„å­ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•æ™‚ã«å‘¼ã³ã€ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼ã« `QueueHandler` ã‚’ã‚»ãƒƒãƒˆã—ã¦å…¨ãƒ­ã‚°ã‚’å…±æœ‰ `Queue` ã«æµã™
3. ãƒ¯ãƒ¼ã‚«é–¢æ•°ã¯é€šå¸¸ã©ãŠã‚Š `logging.getLogger(__name__)` ã‚’ä½¿ã£ã¦ãƒ­ã‚°ã‚’å‡ºã™ã ã‘ã§ã€ã™ã¹ã¦ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚­ãƒ¥ãƒ¼çµŒç”±ã§ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¸å±Šã
4. ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ `QueueListener` ã« `TqdmLoggingHandler` ã‚’æ¸¡ã—ã¦èµ·å‹•ã—ã€`mp.Pool` ç”Ÿæˆæ™‚ã« `initializer` ã¨ã—ã¦ `worker_init` ã‚’æŒ‡å®šã—ã¦ã‚­ãƒ¥ãƒ¼ã‚’æ¸¡ã™ã€‚æœ€å¾Œã« `listener.stop()` ã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹

# ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰

:::details å®Ÿè£…æ–¹æ³•

```python
import logging
import logging.handlers
import multiprocessing as mp
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import numpy as np


class TqdmLoggingHandler(logging.Handler):
    def emit(self, record):
        try:
            msg = self.format(record)
            tqdm.write(msg)
        except Exception:
            self.handleError(record)


def worker_init(log_queue):
    """
    This runs once in each worker process.
    It configures the root logger to send records to the shared Queue.
    """
    queue_handler = logging.handlers.QueueHandler(log_queue)
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    root.handlers.clear()
    root.addHandler(queue_handler)


def my_function(x):
    logger = logging.getLogger(__name__)
    logger.info(f"[worker] start x={x}")
    time.sleep(np.random.uniform(5,10))
    logger.info(f"[worker] end   x={x}")
    return x


def main():
    log_queue = mp.Queue()

    tqdm_handler = TqdmLoggingHandler()

    listener = logging.handlers.QueueListener(
        log_queue,
        tqdm_handler,
        respect_handler_level=True,
    )
    listener.start()

    x_array = range(20)
    try:
        with ProcessPoolExecutor(
            max_workers=4,
            initializer=worker_init,
            initargs=(log_queue,),
        ) as executor:
            futures = [executor.submit(my_function, x_item) for x_item in x_array]
            results = []
            for future in tqdm(
                as_completed(futures),
                total=len(futures),
            ):
                results.append(future.result())

            print("Results:", results)

    finally:
        listener.stop()


if __name__ == "__main__":
    main()
```

:::

:::details ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–

```python
import contextlib
import logging
import logging.handlers
import multiprocessing as mp
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from collections.abc import Iterator
from tqdm import tqdm
import numpy as np


class TqdmLoggingHandler(logging.Handler):
    def emit(self, record):
        try:
            msg = self.format(record)
            tqdm.write(msg)
        except Exception:
            self.handleError(record)


def my_function(x):
    logger = logging.getLogger(__name__)
    logger.info(f"[worker] start x={x}")
    time.sleep(np.random.uniform(5, 10))
    logger.info(f"[worker] end   x={x}")
    return x


@contextlib.contextmanager
def queue_logging_context(
    level: int = logging.INFO,
) -> Iterator[mp.Queue]:
    log_queue: mp.Queue = mp.Queue()
    listener_handler = TqdmLoggingHandler()
    listener = logging.handlers.QueueListener(
        log_queue,
        listener_handler,
        respect_handler_level=True,
    )

    root = logging.getLogger()
    previous_handlers = root.handlers[:]
    previous_level = root.level

    queue_handler = logging.handlers.QueueHandler(log_queue)
    root.handlers.clear()
    root.addHandler(queue_handler)
    root.setLevel(level)

    listener.start()
    try:
        yield log_queue
    finally:
        listener.stop()
        root.handlers = previous_handlers
        root.setLevel(previous_level)


def worker_init(
    log_queue: mp.Queue,
) -> None:
    root = logging.getLogger()
    queue_handler = logging.handlers.QueueHandler(log_queue)
    root.handlers.clear()
    root.addHandler(queue_handler)
    root.setLevel(logging.INFO)


def main():
    x_array = range(20)
    with queue_logging_context() as log_queue:
        with ProcessPoolExecutor(
            max_workers=4,
            initializer=worker_init,
            initargs=(log_queue,),
        ) as executor:
            futures = [executor.submit(my_function, x_item) for x_item in x_array]
            results = []
            for future in tqdm(
                as_completed(futures),
                total=len(futures),
            ):
                results.append(future.result())

            print("Results:", results)


if __name__ == "__main__":
    main()
```

:::

# ãŠã¾ã‘

:::details å­ãƒ—ãƒ­ã‚»ã‚¹ã®å¾…æ©Ÿæ™‚é–“ãŒåŒã˜å ´åˆ
å­ãƒ—ãƒ­ã‚»ã‚¹ã®å¾…æ©Ÿæ™‚é–“ãŒåŒã˜å ´åˆã€ãŸã¾ã«ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãŒæ›´æ–°ã•ã‚Œãªã„ã“ã¨ãŒã‚ã‚‹ã®ã§ãã‚Œã«å¯¾å‡¦

```python
import logging
import logging.handlers
import multiprocessing as mp
import time
from concurrent.futures import Future, ProcessPoolExecutor, as_completed
from tqdm import tqdm


class TqdmLoggingHandler(logging.Handler):
    def emit(self, record):
        try:
            msg = self.format(record)
            tqdm.write(msg)
        except Exception:
            self.handleError(record)


def worker_init(log_queue):
    """
    This runs once in each worker process.
    It configures the root logger to send records to the shared Queue.
    """
    queue_handler = logging.handlers.QueueHandler(log_queue)
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    root.handlers.clear()
    root.addHandler(queue_handler)


def my_function(x):
    logger = logging.getLogger(__name__)
    logger.info(f"[worker] start x={x}")
    time.sleep(5)
    logger.info(f"[worker] end   x={x}")
    return x


def main():
    log_queue = mp.Queue()

    tqdm_handler = TqdmLoggingHandler()

    listener = logging.handlers.QueueListener(
        log_queue,
        tqdm_handler,
        respect_handler_level=True,
    )
    listener.start()

    x_array = range(20)
    try:
        with ProcessPoolExecutor(
            max_workers=4,
            initializer=worker_init,
            initargs=(log_queue,),
        ) as executor:
            results: list = []
            futures: list[Future[int]] = []
            progress = tqdm(total=len(x_array), desc="Processing")
            try:
                for x_item in x_array:
                    future = executor.submit(my_function, x_item)
                    future.add_done_callback(lambda _: progress.update(1))
                    futures.append(future)

                for future in as_completed(futures):
                    results.append(future.result())
            finally:
                progress.close()

        print("Results:", results)

    finally:
        listener.stop()


if __name__ == "__main__":
    main()
```

:::
